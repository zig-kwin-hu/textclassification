02
2
03
5
00
6
01
4
06
3
07
1
04
9
05
7
08
8
09
0
init finish
prepare finish
2016-10-31T04:48:55.739423: step 100, loss 2.11377, acc 0.3125
2016-10-31T05:06:41.491659: step 200, loss 2.48979, acc 0
2016-10-31T05:24:30.391973: step 300, loss 2.4666, acc 0.125
2016-10-31T05:42:28.209703: step 400, loss 1.82186, acc 0.25
2016-10-31T06:00:46.781164: step 500, loss 1.05764, acc 0.6875

Evaluation:
36
0.526041666667
2016-10-31T06:20:18.025252: step 600, loss 0.853596, acc 0.625
2016-10-31T06:38:05.884390: step 700, loss 0.443933, acc 0.875
2016-10-31T06:57:57.283364: step 800, loss 0.321081, acc 0.9375
2016-10-31T07:17:39.323820: step 900, loss 0.0866514, acc 1
2016-10-31T07:36:39.402338: step 1000, loss 0.11623, acc 0.9375

Evaluation:
36
0.847222222222
2016-10-31T07:56:24.350995: step 1100, loss 0.0359193, acc 1
2016-10-31T08:16:13.365901: step 1200, loss 0.0339369, acc 1
2016-10-31T08:36:13.437070: step 1300, loss 0.0049686, acc 1
2016-10-31T09:01:53.044123: step 1400, loss 0.0164436, acc 1
2016-10-31T09:29:20.115096: step 1500, loss 0.0380241, acc 1

Evaluation:
36
0.868055555556
2016-10-31T09:55:17.308389: step 1600, loss 0.00405009, acc 1
2016-10-31T10:19:05.347535: step 1700, loss 0.00865401, acc 1
2016-10-31T10:41:31.682307: step 1800, loss 0.00269655, acc 1
2016-10-31T11:04:07.157655: step 1900, loss 0.00191239, acc 1
2016-10-31T11:27:21.984504: step 2000, loss 0.000742135, acc 1

Evaluation:
36
0.854166666667
2016-10-31T11:52:02.977184: step 2100, loss 0.0470532, acc 1
2016-10-31T12:17:32.544821: step 2200, loss 0.0248836, acc 1
2016-10-31T12:44:00.510324: step 2300, loss 0.00859229, acc 1
2016-10-31T13:10:16.148128: step 2400, loss 0.00814584, acc 1
2016-10-31T13:30:23.200410: step 2500, loss 0.00354628, acc 1

Evaluation:
36
0.869791666667
2016-10-31T13:48:40.933921: step 2600, loss 0.00617055, acc 1
2016-10-31T14:06:00.663059: step 2700, loss 0.000819962, acc 1
2016-10-31T14:23:35.677959: step 2800, loss 0.000693383, acc 1

Evaluation:
36
0.881944444444
02
7
03
9
00
3
01
2
06
8
07
4
04
5
05
0
08
1
09
6
init finish
prepare finish
2016-10-31T22:50:05.398120: step 100, loss 2.02013, acc 0.375
2016-11-01T00:00:00.064405: step 200, loss 2.01621, acc 0.375
2016-11-01T01:22:52.896232: step 300, loss 1.03675, acc 0.6875
2016-11-01T02:21:22.181916: step 400, loss 0.993805, acc 0.6875
2016-11-01T02:59:14.628361: step 500, loss 0.596846, acc 0.8125

Evaluation:
36
0.668402777778
2016-11-01T04:10:54.208325: step 600, loss 0.787143, acc 0.75
2016-11-01T06:45:29.159249: step 700, loss 0.925244, acc 0.6875
2016-11-01T09:35:35.011401: step 800, loss 0.452767, acc 0.875
2016-11-01T12:39:02.999895: step 900, loss 0.402806, acc 0.8125
2016-11-01T15:50:12.707710: step 1000, loss 0.208852, acc 0.9375

Evaluation:
36
0.774305555556
2016-11-01T19:14:26.651255: step 1100, loss 0.559099, acc 0.875
2016-11-01T22:44:45.200067: step 1200, loss 0.0628472, acc 1
2016-11-02T02:10:29.761299: step 1300, loss 0.253038, acc 0.9375
2016-11-02T04:11:48.303425: step 1400, loss 0.126659, acc 0.9375
2016-11-02T05:14:37.686460: step 1500, loss 0.0239687, acc 1

Evaluation:
36
0.8125
2016-11-02T06:30:09.836847: step 1600, loss 0.0359729, acc 1
2016-11-02T08:00:29.358380: step 1700, loss 0.137066, acc 1
2016-11-02T10:40:40.294073: step 1800, loss 0.120779, acc 1
2016-11-02T13:43:17.289258: step 1900, loss 0.0416107, acc 1
2016-11-02T16:37:29.975672: step 2000, loss 0.00795234, acc 1

Evaluation:
36
0.826388888889
2016-11-02T19:07:13.434268: step 2100, loss 0.011611, acc 1
2016-11-02T21:42:05.203728: step 2200, loss 0.00530132, acc 1
2016-11-03T00:21:49.073325: step 2300, loss 0.004462, acc 1
2016-11-03T03:13:48.222446: step 2400, loss 0.182868, acc 0.9375
2016-11-03T06:07:56.578944: step 2500, loss 0.00288387, acc 1

Evaluation:
36
0.819444444444
2016-11-03T09:22:55.603748: step 2600, loss 0.0445528, acc 1
2016-11-03T12:50:47.832333: step 2700, loss 0.0034113, acc 1
2016-11-03T16:23:08.887360: step 2800, loss 0.0167972, acc 1

Evaluation:
36
0.842013888889
